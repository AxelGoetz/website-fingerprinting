\section{Background Information, Related Work and Research}
In the following chapter, we further explore the motivation for undertaking the project,
analyse the current state of the project domain and outline the research that forms the basis for the rest of the report.

\subsection{The Problem}
As previously mentioned, the goal of this project is to automate the feature selection process for a website fingerprinting attack.
By this we mean that given a specific trace, our model should be able to produce a fixed-length vector that is a close representation of the respective trace.
However, before we delve into the details of the attack, we first need to gain a greater understanding of some concepts such as
onion routing, website fingerprinting in general and deep learning.

\subsubsection{Onion Routing}
To preserve privacy, we do not only need to obscure the content of a webpage but also hide who is taking to whom \cite{goldschlag1999onion}.
Tor achieves both of these by making use of a technique, called \textit{onion routing}, which is a very simple protocol that can be divided up into three phases:
connection setup, data movement and connection tear-down \cite{goldschlag1999onion}. We show how it works by taking a simple example of Alice trying to communicate with Bob.

\begin{enumerate}
  \item \textbf{Connection Setup:}
    \begin{itemize}
      \item Alice's Tor client obtains a list of Tor nodes from a directory server.
      \item Then Alice picks three random Tor nodes and labels them \textit{one, two} and \textit{three}.
      \item Alice communicates with the first node and shares a symmetric key.
      \item Next, Alice sends messages to the first node, which are then forwarded to the second node to share another symmetric key to the second node.
      \item Finally, Alice continues sending messages to the first node, which are forwarded to the second and finally to the third node to share the final symmetric key.
        What is important here is that we use a secure key-sharing algorithm such that only Alice and the respective node know the keys.
        Additionally, since all of the traffic is forwarded from the first node, the second and the third nodes do not know the identity of Alice.
    \end{itemize}

    \begin{figure}[h]
      \centering
      \includegraphics[width=0.75\textwidth]{tor_setup}
      \caption{An example of a connection setup for the onion routing protocol.}
      \label{fig:tor_setup}
    \end{figure}

  \item \textbf{Data Movement:}
    \begin{itemize}
      \item Before Alice can send any data, it first needs to encrypt it in different layers. By this we mean that first it encrypts the data \textit{(with Bob's address)}
        using the shared key from the third node. Next, it encrypts that data again \textit{(with the address of the third node)} using the key from the second node.
        Finally, as expected, it encrypts the data a final time \textit{(with the address of the second node)} using the shared key from the first node.

      \item Now Alice is ready to send the data to the first node.
      \item Once the first node received the data, it decrypts it using the shared key. This reveals the address to the second node.
        The key is here that the first node cannot see the data nor where it is going, since that is still encrypted.

      \item Next, the first node forwards the data that it just unencrypted to the second node. Again, this node decrypts the data, revealing the address to the third node
        but now it doesn't know what the data is, where the final destination is or where it originally came from.

      \item Lastly, the second node forwards the data to the third node. After encryption, this final node can see the data and where it is going but it does not know where it came from.
        So it forwards the data to Bob and not a single party should be able to know the data, the final destination and where it originally came from except for Alice and Bob.
    \end{itemize}

    Now we know why the protocol is called onion routing because it encrypts the data in multiple layers and at every node, one of the layers of the onion is peeled off \cite{tor_project2}.
    The key is that none of the nodes know the complete path that has been taken.

    \begin{figure}[h]
      \centering
      \includegraphics[width=0.75\textwidth]{tor_message_sending}
      \caption{Sending a message with the onion routing protocol.}
      \label{fig:tor_message_sending}
    \end{figure}

  \item \textbf{Connection Tear-down:} This can be initiated by any of the nodes or the client and the process is very straightforward.
    Either the client sends a request for a tear-down to the first node to remove any data on the connection (including the shared key), which is then forwarded to the other nodes.
    Or one of the nodes sends a tear-down message to both the previous node and the next node, which are then forwarded in both directions \cite{goldschlag1999onion}.

\end{enumerate}

Tor generally uses the same circuit for connections that happen within around 10 minutes after creating a circuit.
Later requests, however, are given a new circuit \cite{tor_project, tor_project2}.

\subsection{Related Work}

\subsubsection{Website Fingerprinting}

Website fingerprinting (WF) is the process of attempting to identify which web pages a specific client visits by analysing their traffic traces.
Hence, an attacker is considered to be \textit{local} . By this we mean that they can eavesdrop on the traffic between the client and the first Tor node, as shown in figure \ref{fig:threat_model}.
So it can be anyone from a person on the same network to an ISP.
The reason as to why the attacker has to be local is because in onion routing systems, it is the only place in the network where you still know
the true identity of the client.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.75\textwidth]{threat_model}
  \caption{Threat model for a simple website fingerprinting attack.}
  \label{fig:threat_model}
\end{figure}

In general, the attack is as follows. The attacker first collects a dataset that contains packet traces for loading several web pages.
In practice, these web pages are the ones that an attacker is interested in monitoring.
From those traces, the attacker extracts a fixed number of features, the so-called \textit{fingerprint}.
Next, it uses those fingerprints to train some sort of model \textit{(often a machine learning model)} to find patterns in the data.
The attacker observes packet traces, generated by the client, and uses the model to classify which web pages the user is visiting.
Therefore, an attack is essentially a classification problem, where you try to map packet traces into web pages.

We should denote that throughout this report, we will be using the word ``web pages'' rather than ``websites''.
The reasoning behind this is that a website often consists of multiple HTML documents,
which can result in significantly different traffic traces, depending on which document you load.

The term \textit{``website fingerprinting''} was first mentioned by A. Hintz who performed a simple traffic analysis on Safeweb,
an encrypting web proxy that tried to protect user's privacy \cite{hintz2002fingerprinting}. Although the attack was very simple, it and other earlier
works show the possibility that encrypted channels might still leak information about the URL \cite{hintz2002fingerprinting, wagner1996analysis}.
Later, in 2009, the first fingerprinting attack against Tor was executed by Herrmann et al. using a \textit{Multinomial Naive Bayes} model.
They managed to get a relatively high accuracy for several protocols, such as \textit{OpenSSL} and \textit{OpenVPN}, on a small set of 775 web pages.
However on more specialised anonymization networks, the model only achieved a 2.96\% for Tor, which they said was caused by a suboptimal configuration \cite{herrmann2009website}.

Around the same period Y. Shi et al. performed an attack that specifically focused on anonymity systems such as Tor \cite{shi2009fingerprinting}.
Using a cosine similarity, they achieved around 50\% accuracy on an even smaller dataset of only 20 web pages \cite{shi2009fingerprinting}.
The first real threat however, was by A. Panchenko et al. who used a \textit{Support Vector Classifier} (SVC) on the same dataset of 775 web pages as Herrmann et al.
and got a 54.61\% success rate \cite{herrmann2009website, panchenko1}.

- Open vs closed world (all of the above have been closed world)

- Naive Bayes

-Wang (kNN)

- Random forest

Panchenko et al. improved upon their previous attack to create one current state-of-the-art methods. They tested their approach on several datasets,
including the one used by Wang et al. on their k-NN attack, where they got around 92\% accuracy.
In an open-world scenario, on the other hand, they got up to 96\% accuracy.






\subsubsection{Defenses}

In gerenal, website fingerprinting is harder on Tor than other services because it performs several defenses (padding 512 byte), background noise (circuit constrction, SENDME)

Defense at application level (Hayes)

\subsubsection{Critical Analysis}
- Tor version
- Closed world
- Well defined packet traces (It is assumed that the attacker knows where the packet trace of a singlenpage load starts and ends. If the client takes much
    longer to load the next page after the current one is loaded, this assumption can be justified.)
- No other activity (multi-tab browsing)

(Website fingerprinting at Internet scale proves that the attack does not scale in realistic settings)

\subsection{Deep Learning}

\subsection{Automatic Feature Selection}

\subsubsection{Stacked Authoencoder}

\subsubsection{Sequence-to-Sequence Model}

\subsection{Software Libraries}
