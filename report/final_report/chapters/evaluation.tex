% TODO: Remove
\addbibresource{../report.bib}

\chapter{Evaluation and Testing}

In the following section, we outline how we will be evaluating the suggested models.
Next, we will perform that evaluation and present the final results.

\section{Experimental Setup}

Since our deep model requires a large amount of computation, we like to make use of parallelization.
Hence, all of our experiments that involve deep learning have been run on an \textit{Amazon EC2 p2.xlarge} instance.
This VM has a \textit{NVIDIA K80 GPU} with 12 GiB of GPU memory.
This instance was then setup with \textit{CUDA 8} and \textit{cuDNN v5.1} \cite{tensorflow,nvidia_developer_2017}.

The rest of the experiments are run on a 2016 MacBook Pro, with a 2.9GHz Intel Core i5 and 8GB of RAM, running MacOS 10.12.
To make sure that the same Python environment is used on both these machines, we consistently use \textit{Python 3.6} and a \textit{virtual environment} for the python dependencies.

As previously mentioned, the main dataset that will be used is \texttt{GRESCHBACH} but we will also be using some of the data in the \texttt{WANG14} dataset to see how the model performs on data that was recorded under different circumstances.
From both these datasets, we will only be using the preprocessed Tor cells and not the raw TCP traffic data.

For all of the experiments that will be conducted below, we only consider an \textit{open-world scenario}.
This means that the test set will contain unmonitored pages that the sequence-to-sequence model or the classifier have never seen before.
For this to work, we train the models on a large set of monitored web pages but also on a small percentage of unmonitored web pages such the classifiers can distinguish between both.

\section{Evaluation Techniques}

There are several different manners in how we can evaluate the feature selection process.
First of all, we could analyse how the model performs on unseen traces, as it learns.
If the difference between both the training error and the error on an unseen instance increases, the model will clearly be overfitting.

However, these graphs only show us how well the model is at reproducing the trace from a fingerprint but now how well it performs in a WF attack.
For this we need to train a classifier and see how well it performs by using the metrics described in section \ref{sec:classifier-training}.

To be able to compare these fingerprints with hand-picked ones, we could run the classifier with hand-picked features and with the automatically generated ones.
These hand-picked features are often chosen by experts and after a careful analysis of what the most appropriate features are.
Hence, if the classifier with our fingerprints were to get similar results or even outperform the classifiers with the hand-picked features, we know that the fingerprint extraction model has been successful.
For these results to be accurate, we do not change any parameters within the classifiers.
Thus everything, except for the features, stays fixed.

For the classifiers, we pick a small set of five existing models.
We aim to pick models that have had an influence on the WF field whilst also having a variety of different classifiers.
This set includes the two \textit{support vector classifiers} (SVCs) used by Panchenko et al. \cite{panchenko1,panchenko2},
the k-fingerprinting attack, which relies on a \textit{random forest} (RF) used by Hayes et al. \cite{kfingerprinting}
and finally the \textit{k-nearest neighbours} (kNN) classifier used by Wang et al. \cite{wang_cai_johnson_nithyanand_goldberg_2014}.

For all of these models, we extract the exact same features as outlined in the respective papers and compare the performance of our generated features compared to the hand-picked ones.
The code for this feature extraction process can be found in the \texttt{feature\_extraction} module.

We also aim to use the exact same hyperparameters described in the respective papers. More specifically:
\begin{itemize}
  \item \textbf{SVC} \cite{panchenko1} - a \textit{radial basis function} (RBF) kernel with $C = 2^{17}$ and $\gamma = 2^{-19}$.
  \item \textbf{SVC} \cite{panchenko2} - uses the same hyperparameters as in the previous SVC but with different features.
  \item \textbf{RF} \cite{kfingerprinting} - shows that the best accuracy/time tradeoff is made when $k = 3$ and $\textit{num\_trees} = 20$.
  \item \textbf{kNN} \cite{wang_cai_johnson_nithyanand_goldberg_2014} - also shows that the best accuracy/time tradoff is made when $k = 2$ and $k_{\textit{reco}} = 5$.
\end{itemize}

However, we do need to note that this might have an impact on the performance because these parameters have been specifically tuned for the hand-picked features and not for our fingerprints.

\section{Evaluation}
As mentioned in section \ref{sec:fingerprint-extraction-training}, for both models. we need to make a couple design decisions regarding different architectures and learning parameters.

\subsection{Stacked Autoencoder}

\subsubsection{Learning Parameter Tuning}

First, we start by varying the mini-batch sizes from $20$ to $400$ in steps of $20$.
The higher the batch size, the longer it takes before making a weight update and the lower the value, the more noise in the training data.
For instance, as can be seen in figure \ref{fig:varying-batch-sizes}, there is clearly a trend of the training error decreasing over time.
However, since the batch size is low for the first case, there is a higher probability of having a batch where the training error is high for all of the samples within that batch.
Hence, the data will look very noisy.
Whilst the greater the batch size, the less noise there is in the data and therefore the easier it is to spot trends.

\begin{figure}[ht]
  \centering
  \includegraphics[width=\textwidth]{varying-batch-sizes}
  \caption{Scaled MSE over traces processed for different batch sizes in a sequence-to-sequence model.}
  \label{fig:varying-batch-sizes}
\end{figure}

% TODO: batch size, learning rate, MSE, optimizer

\subsubsection{Architecture Tuning}

First, we perform several experiments with different architectures to find the most appropriate activation function.
These show that a \textit{sigmoid} activation function continuously results in better learning.

Next, we aim to examine the amount of hidden layers and their respective sizes that yield the best results for a reconstruction task.

\noindent
\textbf{Hidden Layers}

% TODO: Fill in

\noindent
\textbf{Size of Hidden Layers}

The size of the final hidden layer, represents the amount of features that will be extracted
We do not want these values to be too large since this makes the network more complex and requires more computation.
On the other hand, we do not want these values to be too small either since the network might not be able to learn anything useful.

Moreover, we also need to consider the classifiers.
The more features we introduce, the more time and data they require to learn the classification task.
Whilst if the amount of features is too low, the classifier might not be able to learn how to effectively classify any of the web pages.
Hence, we base the amount of hidden states on the amount of features used in previous WF attacks.

\begin{table}[ht]
  \centering
  \begin{tabular}{ r r } \hline
    \multicolumn{1}{c}{\textbf{Model}} & \multicolumn{1}{c}{\textbf{Features}} \\ \hline
    SVC \cite{panchenko1} & $305$ \\
    SVC \cite{panchenko2} & $104$ \\
    RF \cite{kfingerprinting} & $150$ \\
    kNN \cite{wang_cai_johnson_nithyanand_goldberg_2014} & $3737$ \\
    \hline
  \end{tabular}
  \caption{Amount of features for existing attacks.}
  \label{table:feature-wf-attacks}
\end{table}

Therefore, we vary the amount of features between $100$ and $300$ in steps of $40$.
We aim to pick the sizes of the rest of the layers such that the step size is around the same value.

% TODO: FIll in what is the best?

\subsection{Sequence-to-Sequence Model}

\subsubsection{Learning Parameter Tuning}

We try to aim to get the appropriate values for the learning parameters within a simple encoder and decoder with LSTM cells and $120$ hidden states.

After experimentation, the maximum batch size that our EC2 instance could handle memory-wise is around $400$.
Thus through the rest of the report we will use a mini-batch size of $400$.

Next, we vary the learning rate $\gamma$ from $0.01$ to $0.000001$ with various optimizers (\textit{adam}, \textit{gradient descent} or \textit{RMSProp}) and loss functions (\textit{mean squared error (MSE)} or \textit{absolute loss} (AL)).
After trying a wide variety of different permutations, an \textit{adam optimizer} continuously demonstrated better results.
We expected this since adam optimizers are computationally efficient, require a relatively little amount of memory and tend to perform very well with problems that have a large amount of parameters \cite{kingma2014adam},
which is ideal since our network can be unrolled to a very large lengths.

Next, we also note that the best quality of data compression was achieved with a \textit{MSE loss function} and a learning rate of $0.000002$.
Hence, we set $\lambda = 0.000002$, $b = 200$ and use an adam optimizer with MSE for the rest of our experiments.

Since some of the traces are relatively long, it might be worth cutting the traces after a certain amount of time.
However, to compare which amount of time seems to be the most appropriate, we cannot simply use the learning curve because the shorter the trace, the smaller the error is likely to be.
Therefore, we will cut the traces after $2$, $6$ and $10$ seconds, use these values to train a sequence-to-sequence model and train the binary classifiers on the extracted fingerprints.
Next, we can compare the performance of these classifiers to analyse how much information each part of the trace carries.

% TODO: Add performance

% TODO: Add conclusion

\subsubsection{Architecture Tuning}

Now that we have made a decision on which learning parameters to use, we can start changing the architecture of the sequence-to-sequence model to see which ones yield the best results.
We do need to note that we might slightly deviate from the previously chosen learning parameters for different models due to memory constraints.
But when we do, we will clearly state that this is the case.

\noindent
\textbf{Hidden States}

We first start by examining the amount of hidden states in the network.
These directly affects the size of the fingerprints that will be extracted.
In fact, the amount of features extracted is exactly double the amount of hidden states.
Thus we vary the amount of hidden states between $60$ to $140$ in steps of $20$ to see which ones yield the most appropriate results.

For these experiments we train a sequence-to-sequence model with a unidirectional encoder, LSTM cells and without cutting or reversing the traces.
The training data consists $120,000$ monitored and unmonitored web pages, which are shuffled to avoid overfitting on any specific web page.
We only train the model for one epoch, as we seem to have enough data for the model to converge within that epoch.
Hence, every sample that the model sees in the figure below is one that it has never seen before.
So we can easily determine if the model overfits on specific data.

\begin{figure}[ht]
  \centering
  \includegraphics[width=\textwidth]{varying-hidden-states}
  \caption{MSE over the amount of traces processed for varying hidden states.}
  \label{fig:varying-hidden-states}
\end{figure}

Figure \ref{fig:varying-hidden-states} clearly shows us that the smaller the amount of hidden states, the faster the network seems to learn the reconstruction task.
On the other hand, the higher the amount of states, the lower the final error seems to be.
Since we aim to compromise between computational complexity and the time it takes to train the model, around $100$ hidden states seems to be the most appropriate since it manages to a MSE that is very close to the networks with a higher amount of hidden states.

\noindent
\textbf{Bidirectional}

For these experiments, we consider a smaller range of hidden state values from $80$ to $120$ in steps of $20$.
Again, for all of these we will be using LSTM cells without cutting or reversing the traces and all of the learning parameters described above and the training set will remain exactly the same.

% TODO: Add figure + conclusion

\noindent
\textbf{LSTM or GRU Cells}

Here, we train a sequence-to-sequence model with both a unidirectional and bidirectional encoder.
These will both have GRU cells with $100$ and $TODO: Fill in$ hidden states respectively.

Furthermore, we recreate the exact same training conditions as previously.
This means that we will not be cutting the traces nor reversing them whilst training on the same dataset of $120,000$ web pages with the previously chosen learning parameters.

% TODO: Add figure + conclusion

\subsection{Classifier Performance}

We have previously analysed the models' performance based on how well it copies the original input from a fingerprint.
But to examine how well our model performs at automatically generating fingerprints, we compare its performance on different existing classifiers with hand-picked features.
This means that we choose a set of existing WF attacks and recreate them.
Next we run the exact same attack but with the automatically generated features and various scoring statistics.

Note that our results might be slightly lower than in their respective papers since we do not aim to recreate the full attack.
Rather than optimizing different hyperparameters, we aim to use these classifiers and the hand-picked features as an indicator as to how well the fingerprint extraction models perform.

We expect that the automatically generated features will perform worse than the hand-picked ones due to the complexity of the task.
To effectively compress a variable-length trace into a fixed-length representation is a very difficult to achieve task and due to the depth of the network, we expect that it got stuck in a local minima.
However, we still hope to show that it is in fact possible to automate this feature selection process.

As mentioned in section \ref{sec:threat-model}, there are two main threat models that we need to consider.
The first one is a binary classification task, where the adversary wants to see whether or not a user is visiting any webpages within a given set.
The other threat model involves the adversary having a set of monitored pages, and it wants to know which specific pages the user is visiting in that set.
Hence, it is a multiclass classification problem.

Although there are different techniques for evaluating binary and multiclass classification models, we will only use the scoring statistics outlined in section \ref{sec:classifier-training}.
This allows us for easy comparisons between the different threat models.
We do expect that the binary classification models will perform better than the multiclass ones due to the larger amount of options available.

Aforementioned, we have already selected a total of four different existing attacks.
We will refer to the first SVC attack by Panchenko et al. \cite{panchenko1} as \texttt{svc1} and the second one \cite{panchenko2} as \texttt{svc2}.
Whilst we refer the k-fingerprinting attack by Hayes et al. \cite{kfingerprinting} as \texttt{RF} and finally the attack by Wang et al. \cite{wang_cai_johnson_nithyanand_goldberg_2014} as \texttt{kNN}.

\subsubsection{Binary Classification}

We first start by analysing the simplest threat model, namely binary classification.
For all of the models below, we aimed to extract the exact same hand-picked features as were described in the respective papers to the best of our knowledge.

For training these models, we use an extract from the \texttt{GRESCHBACH} dataset with a total of $100$ monitored web pages with $70$ instances each and $5000$ unmonitored web pages.
We then split this set into a training and validation set using a stratified split.
The training set will contain $90\%$ of the monitored web pages whilst we vary the amount of unmonitored pages to see how the models perform.

After the set is split up into a training and validation set, we perform a \textit{stratified k-fold validation} with $k = 3$ on the training set
Then finally we train the classifiers on all of the training data and evaluate them on the test set.

The results for the k-fold validation on the training set for the hand-picked features are outlined in table \ref{table:hand-picked-bin}.
Here, we used a total of $10\%$ of the unmonitored data for training.

\begin{table}[ht]
  \centering
  \begin{tabular}{ r  r  r  r  r  r } \hline
    \multicolumn{1}{c}{\textbf{Model}} & \multicolumn{1}{c}{\textbf{Accuracy}} & \multicolumn{1}{c}{\textbf{BDR}} & \multicolumn{1}{c}{\textbf{TPR}} &
      \multicolumn{1}{c}{\textbf{FPR}} & \multicolumn{1}{c}{\textbf{F1}} \\ \hline

    \texttt{svc1} & $0.91 \pm 0.003$ & $0.99 \pm 0.001$ & $0.97 \pm 0.001$ & $0.07 \pm 0.002$ & $0.90 \pm 0.005$ \\

    \texttt{svc2} & $0.91 \pm 0.008$ & $0.99 \pm 0.001$ & $0.95 \pm 0.003$ & $0.06 \pm 0.004$ & $0.90 \pm 0.008$ \\

    \texttt{RF} & $0.93 \pm 0.003$ & $0.99 \pm 0.001$ & $0.97 \pm 0.006$ & $0.05 \pm 0.003$ & $0.92 \pm 0.005$ \\

    \texttt{kNN} & $0.90 \pm 0.007$ & $0.97 \pm 0.003$ & $0.94 \pm 0.004$ & $0.09 \pm 0.002$ & $0.89 \pm 0.004$ \\

    \hline
  \end{tabular}
  \caption{Performance statistics hand-picked features on a binary classification task with k-fold validation.} % 10 % unmonitored
  \label{table:hand-picked-bin}
\end{table}

As expected, the results on the individual folds is relatively high.
However, now we will measure their performance when training the classifiers on the full training set and evaluating on the validation set, whilst changing the amount of unmonitored pages we train the model on.
Clearly, the models suffer if we introduce a large amount of unmonitored pages in the test set.
But the more unmonitored instances we train on, the better the hand-picked features seem to perform.

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.8\textwidth]{bin-varying-unmon}
  \caption{Varying the amount of unmonitored pages trained on for hand-picked features.}
  \label{fig:bin-varying-unmon}
\end{figure}

% TODO: Automatically generated features
% Bar chart?

\subsubsection{Multiclass Classification}

The multiclass classification scenario is slightly more complex due to the larger array of options.
Hence, we also expect considerably lower results, especially on the test set with a large amount of unmonitored pages.

\begin{table}[ht]
  \centering
  \begin{tabular}{ r  r  r  r  r  r } \hline
    \multicolumn{1}{c}{\textbf{Model}} & \multicolumn{1}{c}{\textbf{Accuracy}} & \multicolumn{1}{c}{\textbf{BDR}} & \multicolumn{1}{c}{\textbf{TPR}} &
      \multicolumn{1}{c}{\textbf{FPR}} & \multicolumn{1}{c}{\textbf{F1}} \\ \hline

    \texttt{svc1} & $0.57 \pm 0.013$ & $0.99 \pm 0.001$ & $0.59 \pm 0.014$ & $0.08 \pm 0.004$ & $0.58 \pm 0.012$ \\

    \texttt{svc2} & $0.59 \pm 0.007$ & $0.99 \pm 0.001$ & $0.61 \pm 0.007$ & $0.07 \pm 0.007$ & $0.60 \pm 0.009$ \\

    \texttt{RF} & $0.59 \pm 0.011$ & $0.99 \pm 0.001$ & $0.58 \pm 0.011$ & $0.02 \pm 0.004$ & $0.63 \pm 0.012$\\

    \texttt{kNN} & $0.58 \pm 0.015$ & $0.97 \pm 0.006$ & $0.56 \pm 0.008$ & $0.09 \pm 0.005$ & $0.57 \pm 0.013$ \\

    \hline
  \end{tabular}
  \caption{Performance statistics hand-picked features on a multiclass classification task.}
  \label{table:mult-handpicked-test-error}
\end{table}

Table \ref{table:mult-handpicked-test-error} shows that the performance does indeed drop on the multiclass classification task.
However, the models seem to perform generally well on the test set.
Especially when given enough unmonitored pages to train them on, as can be seen in figure \ref{fig:mult-varying-unmon}.

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.8\textwidth]{mult-varying-unmon}
  \caption{Varying the amount of unmonitored pages trained on for hand-picked features.}
  \label{fig:mult-varying-unmon}
\end{figure}

\subsubsection{Different Circumstances}

Beside analysing how the fingerprint extraction models perform on data that was recorded under the same circumstances, it would be interesting to examine how it performs on other data.
It has already been shown that the performance of the classifiers is greatly impacted by the network, time and a different TBB version.
But that doesn't necessarily mean that our fingerprint extraction model is impacted similarly.

If our models are not impacted by these flaws, an adversary would only need to train the model once and then it could continue to use it and only retrain the classifiers.
Hence, it would be some sort of \textit{transfer learning}
.
To test this premise, we use the models that we previously trained on the same $120,000$ web pages within the \texttt{GRESCHBACH} dataset.
More specifically, % TODO: State architecture.
Next, we extract the fingerprints from the Tor cells within the \texttt{WANG14} dataset using this model, train a set of classifiers on these fingerprints and note down their performance.

% TODO: Include learning curve

% TODO: Classifier analysis

% TODO: Conclusion

\section{Unit Tests}

On top of evaluating the results, we also needed to ensure that the results were in fact the the code behaves as we expect it to.
For this we use unit tests.
Some of the code, such as the Tensorflow models is difficult to test but we can still test all of the preprocessing to see if that is correct.
For this we use Python's standard \texttt{unittest} module \cite{python_unittest_documentation}.
The reason for this choice is that it is flexible and the standard unit testing framework, which means it is commonly used.

On top of unit tests, \textit{Travis} was also used \cite{travis}.
Travis is a popular tool, that has an easy integration with Github, for continuous integration.
Therefore, every time a commit is pushed to the remote repository, Travis runs all of the tests automatically.
If one of the tests fails, Travis then automatically notifies all the contributors.

Finally, to check if our tests cover our entire codebase, \textit{codecov} is used \cite{codecov}.
This tool automatically checks how much of the codebase all of the unit tests cover.
At the time of writing, the coverage is $93\%$.
The bits that aren't covered by unit tests, such as the Tensorflow implementation of the sequence-to-sequence model, have been carefully examined to see if they behave as expected by using the Tensorflow debugger \cite{tensorflow}.
