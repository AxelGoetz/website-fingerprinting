\chapter{Evaluation and Testing}

In the following section, we outline how we will be evaluating and testing the suggested models.
Next, we will perform that evaluation and present the final results.

\section{Experimental Setup}

Since our deep model requires a large amount of computation, we like to make use of parallelization.
Hence, all of our experiments that involve deep learning will be run on an \textit{Amazon EC2 p2.xlarge} instance.
This VM has a \textit{NVIDIA K80 GPU} with 12 GiB of GPU memory.
All of the instances used were setup with both \textit{CUDA 8} and \textit{cuDNN v5.1} \cite{tensorflow,nvidia_developer_2017}.

The rest of the experiments are run on a 2016 MacBook Pro, with a 2.9GHz Intel Core i5 and 8GB of RAM, running MacOS 10.12.
To make sure that the same Python environment is used on both these machines, we consistently use \textit{Python 3.6} and a \textit{virtual environment} for the python dependencies.

As previously mentioned, the main dataset used is \texttt{GRESCHBACH} but we will also be using some of the data in the \texttt{WANG14} dataset to see how the model performs on data that was recorded under different circumstances.
For both these datasets, we will only be using the preprocessed Tor cells and not the raw TCP traffic data.

Finally, in all of the experiments that are be conducted below, we only consider an \textit{open-world scenario}.
This means that the test set will contain both monitored and unmonitored pages that the fingerprint extraction models and the classifier have never seen before.
For this to work, we train the models on a large set of monitored web pages but also on a small percentage of unmonitored web pages such the classifiers can distinguish between both.

\section{Evaluation Techniques}

There are several different manners in which we can evaluate the feature selection process.
First of all, we could analyse how the model performs on unseen traces, as it learns.
If the difference between both the training error and the error on an unseen instance increases, the model will clearly be overfitting.

However, this data only show us how well the model is at reproducing the trace from a fingerprint but not how well the fingerprints perform in a WF attack.
For this we need to train a classifier and see how well it performs by using the metrics described in section \ref{sec:classifier-training}.

To be able to compare these fingerprints with hand-picked ones, we could train the classifiers with the hand-picked features and with the automatically generated ones.
These hand-picked features are often chosen by experts and after a careful analysis.
Hence, if the classifier with our fingerprints were to get similar results or even outperform the classifiers with the hand-picked features, we know that the fingerprint extraction model has been successful.
For these results to be accurate, we do not change any (hyper)parameters within the classifiers.
Thus everything, except for the features, remains the same.

For the classifiers, we pick a small set of four existing models.
We aim to pick models that have had an influence on the WF field whilst also having a variety of different classifiers.
This set includes the two \textit{support vector classifiers} (SVCs) used by Panchenko et al. \cite{panchenko1,panchenko2},
the k-fingerprinting attack, which relies on a \textit{random forest} (RF) used by Hayes et al. \cite{kfingerprinting}
and finally the \textit{k-nearest neighbours} (kNN) classifier used by Wang et al. \cite{wang_cai_johnson_nithyanand_goldberg_2014}.

For all of these models, we extract the exact same features as outlined in the respective papers.
The code for this feature extraction process can be found in the \texttt{feature\_extraction} module.

We also aim to use the exact same hyperparameters described in the respective papers. More specifically:
\begin{itemize}
  \item \textbf{SVC} \cite{panchenko1} - a \textit{radial basis function} (RBF) kernel with $C = 2^{17}$ and $\gamma = 2^{-19}$.
  \item \textbf{SVC} \cite{panchenko2} - uses the same hyperparameters as in the previous SVC but with different features.
  \item \textbf{RF} \cite{kfingerprinting} - a good accuracy/time tradeoff when $k = 3$ and $\textit{num\_trees} = 20$.
  \item \textbf{kNN} \cite{wang_cai_johnson_nithyanand_goldberg_2014} - also has a good accuracy/time tradoff when $k = 2$ and $k_{\textit{reco}} = 5$.
\end{itemize}

We do need to note that these parameters have been specifically tuned for the hand-picked features and not for our fingerprints.

\section{Evaluation}
As mentioned in section \ref{sec:fingerprint-extraction-training}, for both deep learning models. we need to make a couple design decisions regarding different architectures and learning parameters.
We perform several experiments here to see which ones are the most appropriate.

\subsection{Stacked Autoencoder}

\subsubsection{Learning Parameter Tuning}

First, we start by varying the mini-batch sizes from $20$ to $600$ in steps of $20$ for a simple model with an input layer of $3000$ cells, and two hidden layers with $1600$ and $200$ neurons respectively.
The higher the batch size, the longer it takes before making a weight update and the lower the value, the more noise in the training data.
We notice that a total batch size of $400$ seems to be a good tradeoff.

Next, a \textit{mean squared error} (MSE) loss function with an \textit{RMSProp} optimizer and a $0.01$ learning rate and seem to yield the most appropriate results.
Finally, we also use batch normalization for all experiments since it allows the model for faster convergence.

\subsubsection{Architecture Tuning}

Our experiments show that a \textit{sigmoid} activation function continuously results in better learning with a variety of different hidden layers with different sizes.

The amount of hidden layers is a slightly more difficult decision.
Since we want the simplest network possible that is able to learn a representation
Hence, we experiment with networks with a total of $1$ up to $3$ hidden layers.
For each of these, the input layer will consist of $3000$ nodes and we will attempt to extract $200$ features, which means that the sizes of the hidden layers will gradually decrease to $200$ neurons.

\begin{figure}[ht]
  \centering
  \includegraphics[width=\textwidth]{changing-depth}
  \caption{Learning curves for changing the depth of the stacked autoencoder}
  \label{fig:changing-depth}
\end{figure}

Figure \ref{fig:changing-depth} already shows us that a network with two hidden layers provides a good complexity/training error tradeoff.
Now that we know the depth of the network, we also need to consider changing the size of the final hidden layer since it represents the amount of features that will be extracted.
The more features we introduce, the more time and data we require to learn the classification task.
Whilst if the amount of features is too low, the classifiers might not be able to learn how to effectively classify any of the web pages.
Hence, we base the size of the final state on the amount of features used in previous WF attacks.

\begin{table}[ht]
  \centering
  \begin{tabular}{ r r } \hline
    \multicolumn{1}{c}{\textbf{Model}} & \multicolumn{1}{c}{\textbf{Features}} \\ \hline
    SVC \cite{panchenko1} & $305$ \\
    SVC \cite{panchenko2} & $104$ \\
    RF \cite{kfingerprinting} & $150$ \\
    kNN \cite{wang_cai_johnson_nithyanand_goldberg_2014} & $3737$ \\
    \hline
  \end{tabular}
  \caption{Amount of features for existing attacks.}
  \label{table:feature-wf-attacks}
\end{table}

Based on table \ref{table:feature-wf-attacks}, we vary the amount of features between $100$ and $300$ in steps of $50$.
From figure \ref{fig:changing-output-size}, we determine that around $200$ nodes provides us with the best tradeoff.
Therefore, throughout the rest of the report when we refer to a stacked autoencoder, its architecture consists of $3$ layers with $3000$, $1600$ and $200$ nodes each.

\begin{figure}[ht]
  \centering
  \includegraphics[width=\textwidth]{changing-output-size}
  \caption{Learning curves for changing the size of the middle layer in the stacked autoencoder}
  \label{fig:changing-output-size}
\end{figure}

\newpage

\subsection{Sequence-to-Sequence Model}

\subsubsection{Learning Parameter Tuning}

We try to aim to get the appropriate values for the learning parameters within a simple encoder and decoder with LSTM cells and $120$ hidden states.

After experimentation, the maximum batch size that our EC2 instance could handle memory-wise is around $400$.
Thus through the rest of the report we will use a mini-batch size of $400$.

Next, we vary the learning rate $\gamma$ from $0.01$ to $0.000001$ with various optimizers (\textit{adam}, \textit{gradient descent} or \textit{RMSProp}) and loss functions (\textit{mean squared error (MSE)} or \textit{absolute loss} (AL)).
After trying a wide variety of different permutations, an \textit{adam optimizer} continuously demonstrated better results.
We already expected this since adam optimizers are computationally efficient, require relatively little amount of memory and tend to perform very well with problems that have a large amount of parameters \cite{kingma2014adam},
which is ideal since our network can be unrolled to a very large lengths.

Next, we also note that the best quality of data compression was achieved with a \textit{MSE loss function} and a learning rate of $0.000002$.
Hence, we set $\lambda = 0.000002$, $b = 400$ and use an adam optimizer with a MSE loss function for the rest of our experiments.

Since some of the traces are relatively long, it might be worth cutting the them after a certain amount of time.
However, to compare after which time to cut the trace, we cannot simply base our analysis on the learning curve because the shorter the trace, the smaller the error is likely to be.
Therefore, we will cut the traces after $2$, $6$ and $10$ seconds, use these values to train a sequence-to-sequence model and train binary classifiers on the extracted fingerprints.
Next, we can compare the performance of these classifiers to analyse how much information each part of the trace carries.

\begin{figure}[ht]
  \centering
  \includegraphics[width=\textwidth]{trace-cutting}
  \caption{Average performance measures of all classifiers after cutting traces.}
  \label{fig:trace-cutting}
\end{figure}

Figure \ref{fig:trace-cutting} shows us that majority of the information is in fact carried in the first couple of seconds of the trace.
Hence, for the rest of our experiments we will be cutting the traces after $10$ seconds.

Finally, we also use batch normalization for all experiments since it allows the model for faster convergence.

\subsubsection{Architecture Tuning}

Now that we have made a decision on which learning parameters to use, we can start changing the architecture of the sequence-to-sequence model to see which ones yield the best results.

\noindent
\textbf{Hidden States}

We first start by examining the amount of hidden states in the network.
These directly affect the size of the fingerprints that will be extracted.
In fact, the amount of features extracted is exactly double the amount of hidden states.
Thus, based on table \ref{table:feature-wf-attacks}, we vary the amount of hidden states between $60$ to $140$ in steps of $20$ to see which ones yield the most appropriate results.

For these experiments we train a sequence-to-sequence model with a unidirectional encoder, LSTM cells and without cutting or reversing the traces.
The training data consists $120,000$ monitored and unmonitored web pages, which are shuffled to avoid overfitting on any specific web page.
We only train the model for one epoch, as we seem to have enough data for the model to converge within that epoch.
Hence, every sample that the model sees in the figure below is one that it has never seen before.
So we can easily determine that the model is not overfitting.

\begin{figure}[ht]
  \centering
  \includegraphics[width=\textwidth]{varying-hidden-states}
  \caption{MSE over the amount of traces processed for varying hidden states.}
  \label{fig:varying-hidden-states}
\end{figure}

Figure \ref{fig:varying-hidden-states} clearly shows us that the smaller the amount of hidden states, the faster the network seems to learn the reconstruction task.
On the other hand, the higher the amount of states, the lower the final error seems to be.
Since we aim to compromise between computational complexity and the time it takes to train the model, around $100$ hidden states seems to be the most appropriate.

\newpage

\noindent
\textbf{Bidirectional}

For these experiments, we consider a smaller range of hidden state values from $80$ to $120$ in steps of $20$.
Again, for all of these we will be using LSTM cells without cutting or reversing the traces, with all of the learning parameters described above and the exact same training set used in the previous experiment.

\begin{figure}[ht]
  \centering
  \includegraphics[width=\textwidth]{varying-hidden-states2}
  \caption{MSE over the amount of traces processed for varying hidden states for a bidirectional encoder.}
  \label{fig:varying-hidden-states2}
\end{figure}

\noindent
As can be seen in figure \ref{fig:varying-hidden-states2}, around $80$ hidden states seems to provide the best complexity/error tradeoff.

\newpage

\noindent
\textbf{LSTM or GRU Cells}

Here, we train a sequence-to-sequence model with both a unidirectional and bidirectional encoder.
These will both have GRU cells with $100$ and $80$ hidden states respectively.
Furthermore, we recreate the exact same training conditions as in the previous experiments.

\begin{figure}[ht]
  \centering
  \includegraphics[width=\textwidth]{GRUcell}
  \caption{Learning curves for different cell types.}
  \label{fig:varying-cell-type}
\end{figure}

As can be seen in figure \ref{fig:varying-cell-type}, all of the different architectures converge to a similar value.
Although GRU cells seem to converge faster, they are also slightly more unstable especially around batch $80$ to $100$.
The most stable model seems to be the bidirectional encoder with LSTM cells.
Although it has more parameters than the unidirectional encoder models, it takes a very similar amount of time to train it.
Hence, throughout the rest of the report when we refer to a sequence-to-sequence model, its architecture consists of a bidirectional encoder with $80$ hidden states.

\subsection{Classifier Performance}

We have previously analysed the models' performance based on how well it copies the original input from a fingerprint.
But to examine how well our model performs during a real WF attack, we compare its performance on different existing classifiers with hand-picked features.
This means that we choose a set of existing WF attacks and recreate them.
Next we run the exact same attack but with both the hand-picked and automatically generated features.

Note that our results might be slightly lower than in their respective papers since we do not aim to recreate the full attack.
Rather than optimizing different hyperparameters, we aim to use these classifiers and the hand-picked features as an indicator as to how well the fingerprint extraction models perform.

We expect that the automatically generated features will perform worse than the hand-picked ones due to the complexity of the task.
However, we still hope to show that it is in fact possible to automate this feature selection process till a certain extent.

As mentioned in section \ref{sec:threat-model}, there are two main threat models that we need to consider.
The first one is a binary classification task, where the adversary wants to see whether or not a user is visiting any webpages within a given set.
Whilst the other threat model involves the adversary having a set of monitored pages, and wants to know which specific pages the user is visiting in that set.
Hence, it is a multiclass classification problem.

Although there are different techniques for evaluating binary and multiclass classification models, we will only use the scoring statistics outlined in section \ref{sec:classifier-training}.
This allows us for easy comparisons between the different threat models.
We do expect that the binary classification models will perform better than the multiclass ones due to the smaller amount of options available.

Aforementioned, we have already selected a total of four different existing attacks.
We will refer to the first SVC attack by Panchenko et al. \cite{panchenko1} as \texttt{svc1} and the second one \cite{panchenko2} as \texttt{svc2}.
Whilst we refer the k-fingerprinting attack by Hayes et al. \cite{kfingerprinting} as \texttt{RF} and finally the attack by Wang et al. \cite{wang_cai_johnson_nithyanand_goldberg_2014} as \texttt{kNN}.

\subsubsection{Binary Classification}

We first start by analysing the simplest threat model, namely binary classification.
For all of the models below, we aim to extract the exact same hand-picked features as were described in the respective papers to the best of our knowledge.

For training these models, we use an extract from the \texttt{GRESCHBACH} dataset with a total of $100$ monitored web pages with $70$ instances each and $5000$ unmonitored web pages.
We then split this set into a training and validation set using a stratified split.
The training set will contain $90\%$ of the monitored web pages whilst we vary the amount of unmonitored pages to see how the models perform.

After the set is split up into a training and validation set, we perform a \textit{stratified k-fold validation} with $k = 3$ on the training set
Then finally we train the classifiers on all of the training data and evaluate them on the test set.

The results for the k-fold validation on the training set for the hand-picked features are outlined in table \ref{table:hand-picked-bin}.
Here, we used a total of $10\%$ of the unmonitored data for training.
As expected, the results with a small amount of unmonitored data is relatively high.

\begin{table}[ht]
  \centering
  \begin{tabular}{ r  r  r  r  r  r } \hline
    \multicolumn{1}{c}{\textbf{Model}} & \multicolumn{1}{c}{\textbf{Accuracy}} & \multicolumn{1}{c}{\textbf{BDR}} & \multicolumn{1}{c}{\textbf{TPR}} &
      \multicolumn{1}{c}{\textbf{FPR}} & \multicolumn{1}{c}{\textbf{F1}} \\ \hline

    \texttt{svc1} & $0.91 \pm 0.003$ & $0.99 \pm 0.001$ & $0.97 \pm 0.001$ & $0.07 \pm 0.002$ & $0.90 \pm 0.005$ \\

    \texttt{svc2} & $0.91 \pm 0.008$ & $0.99 \pm 0.001$ & $0.95 \pm 0.003$ & $0.06 \pm 0.004$ & $0.90 \pm 0.008$ \\

    \texttt{RF} & $0.93 \pm 0.003$ & $0.99 \pm 0.001$ & $0.97 \pm 0.006$ & $0.05 \pm 0.003$ & $0.92 \pm 0.005$ \\

    \texttt{kNN} & $0.88 \pm 0.007$ & $0.99 \pm 0.003$ & $0.97 \pm 0.004$ & $0.10 \pm 0.002$ & $0.94 \pm 0.004$ \\

    \hline
  \end{tabular}
  \caption{Performance statistics hand-picked features on a binary classification task with k-fold validation whilst training on $10\%$ of the unmonitored pages.}
  \label{table:hand-picked-bin}
\end{table}

Next, we will be analyzing the performance of these classifiers with the automatically generated features.
We do note that from here on we refer to \texttt{svc1} and \texttt{svc2} as \texttt{svc} since they both have the same hyperparameters but were trained on different hand-picked features.

\begin{table}[ht]
  \centering
  \begin{tabular}{ r  r  r  r  r  r } \hline
    \multicolumn{1}{c}{\textbf{Model}} & \multicolumn{1}{c}{\textbf{Accuracy}} & \multicolumn{1}{c}{\textbf{BDR}} & \multicolumn{1}{c}{\textbf{TPR}} &
      \multicolumn{1}{c}{\textbf{FPR}} & \multicolumn{1}{c}{\textbf{F1}} \\ \hline

    \texttt{svc} & $0.92 \pm 0.001$ & $0.99 \pm 0.001$ & $0.98 \pm 0.001$ & $0.07 \pm 0.002$ & $0.89 \pm 0.003$ \\

    \texttt{RF} & $0.77 \pm 0.012$ & $0.76 \pm 0.004$ & $0.87 \pm 0.009$ & $0.15 \pm 0.004$ & $0.86 \pm 0.007$ \\

    \texttt{kNN} & $0.74 \pm 0.010$ & $0.73 \pm 0.009$ & $0.85 \pm 0.004$ & $0.18 \pm 0.007$ & $0.84 \pm 0.009$ \\

    \hline
  \end{tabular}
  \caption{Performance statistics autoencoder features on a binary classification task with k-fold validation whilst training on $10\%$ of the unmonitored pages.}
  \label{table:ae-bin}
\end{table}

\begin{table}[ht]
  \centering
  \begin{tabular}{ r  r  r  r  r  r } \hline
    \multicolumn{1}{c}{\textbf{Model}} & \multicolumn{1}{c}{\textbf{Accuracy}} & \multicolumn{1}{c}{\textbf{BDR}} & \multicolumn{1}{c}{\textbf{TPR}} &
      \multicolumn{1}{c}{\textbf{FPR}} & \multicolumn{1}{c}{\textbf{F1}} \\ \hline

    \texttt{svc} & $0.93 \pm 0.001$ & $0.99 \pm 0.001$ & $0.99 \pm 0.001$ & $0.06 \pm 0.003$ & $0.90 \pm 0.002$ \\

    \texttt{RF} & $0.86 \pm 0.004$ & $0.99 \pm 0.001$ & $0.83 \pm 0.003$ & $0.07 \pm 0.005$ & $0.88 \pm 0.003$ \\

    \texttt{kNN} & $0.81 \pm 0.008$ & $0.95 \pm 0.007$ & $0.97 \pm 0.007$ & $0.14 \pm 0.012$ & $0.89 \pm 0.009$ \\

    \hline
  \end{tabular}
  \caption{Performance statistics sequence-to-sequence features on a binary classification task with k-fold validation whilst training on $10\%$ of the unmonitored pages.}
  \label{table:seq2seq-bin}
\end{table}

Both table \ref{table:ae-bin} and \ref{table:seq2seq-bin} show that the performance on a small amount of unmonitored pages is very similar with the \texttt{svc} model but slightly lower for both the \texttt{RF} and \texttt{kNN} attacks.

\newpage

Now we will the performance when training the classifiers on the full training set and evaluating them on the validation set, whilst changing the amount of unmonitored pages we train the model on.
Clearly, figure \ref{fig:bin-unmon-performance} shows us that the models suffer if we introduce a large amount of unmonitored pages in the test set.
But the more unmonitored instances we train on, the better the classifiers seem to perform.

Additionally, figure \ref{fig:bin-unmon-performance} also shows that the \texttt{RF} classifier seems to perform best whilst training in on both the hand-picked and automatically generated features.
Next, we also note that the hand-picked features currently still get the best overall performance, followed by the sequence-to-sequence features and the autoencoder features.

\begin{figure}[ht]
  \centering
  \includegraphics[width=\textwidth]{bin-unmon-performance}
  \caption{Varying the amount of unmonitored pages trained on for different features.}
  \label{fig:bin-unmon-performance}
\end{figure}

\subsubsection{Multiclass Classification}

The multiclass classification scenario is slightly more complex due to the larger array of options.
Hence, we also expect considerably lower results, especially on the test set with a large amount of unmonitored pages.

\begin{table}[ht]
  \centering
  \begin{tabular}{ r  r  r  r  r  r } \hline
    \multicolumn{1}{c}{\textbf{Model}} & \multicolumn{1}{c}{\textbf{Accuracy}} & \multicolumn{1}{c}{\textbf{BDR}} & \multicolumn{1}{c}{\textbf{TPR}} &
      \multicolumn{1}{c}{\textbf{FPR}} & \multicolumn{1}{c}{\textbf{F1}} \\ \hline

    \texttt{svc1} & $0.57 \pm 0.013$ & $0.99 \pm 0.001$ & $0.59 \pm 0.014$ & $0.08 \pm 0.004$ & $0.70 \pm 0.012$ \\

    \texttt{svc2} & $0.59 \pm 0.007$ & $0.99 \pm 0.001$ & $0.61 \pm 0.007$ & $0.07 \pm 0.007$ & $0.72 \pm 0.009$ \\

    \texttt{RF} & $0.59 \pm 0.011$ & $0.99 \pm 0.001$ & $0.58 \pm 0.011$ & $0.02 \pm 0.004$ & $0.72 \pm 0.012$\\

    \texttt{kNN} & $0.55 \pm 0.015$ & $0.92 \pm 0.006$ & $0.55 \pm 0.008$ & $0.09 \pm 0.005$ & $0.69 \pm 0.013$ \\

    \hline
  \end{tabular}
  \caption{Performance statistics hand-picked features on a multiclass classification task with k-fold validation whilst training on $10\%$ of the unmonitored pages.}
  \label{table:mult-handpicked-test-error}
\end{table}

Table \ref{table:mult-handpicked-test-error} shows that the performance does indeed drop on the multiclass classification task.
On the other hand, both table \ref{table:mult-ae-test-error} and \ref{table:mult-seq2seq-test-error} show that the performance for automatically generated features is even lower than the hand-picked ones.

\begin{table}[ht]
  \centering
  \begin{tabular}{ r  r  r  r  r  r } \hline
    \multicolumn{1}{c}{\textbf{Model}} & \multicolumn{1}{c}{\textbf{Accuracy}} & \multicolumn{1}{c}{\textbf{BDR}} & \multicolumn{1}{c}{\textbf{TPR}} &
      \multicolumn{1}{c}{\textbf{FPR}} & \multicolumn{1}{c}{\textbf{F1}} \\ \hline

    \texttt{svc} & $0.22 \pm 0.003$ & $0.54 \pm 0.002$ & $0.17 \pm 0.002$ & $0.16 \pm 0.004$ & $0.29 \pm 0.004$ \\

    \texttt{RF} & $0.25 \pm 0.009$ & $0.62 \pm 0.003$ & $0.18 \pm 0.008$ & $0.13 \pm 0.007$ & $0.30 \pm 0.009$\\

    \texttt{kNN} & $0.20 \pm 0.015$ & $0.48 \pm 0.006$ & $0.17 \pm 0.006$ & $0.20 \pm 0.007$ & $0.28 \pm 0.011$ \\

    \hline
  \end{tabular}
  \caption{Performance statistics autoencoder features on a multiclass classification task with k-fold validation whilst training on $10\%$ of the unmonitored pages.}
  \label{table:mult-ae-test-error}
\end{table}

\begin{table}[ht]
  \centering
  \begin{tabular}{ r  r  r  r  r  r } \hline
    \multicolumn{1}{c}{\textbf{Model}} & \multicolumn{1}{c}{\textbf{Accuracy}} & \multicolumn{1}{c}{\textbf{BDR}} & \multicolumn{1}{c}{\textbf{TPR}} &
      \multicolumn{1}{c}{\textbf{FPR}} & \multicolumn{1}{c}{\textbf{F1}} \\ \hline

    \texttt{svc} & $0.35 \pm 0.004$ & $0.69 \pm 0.008$ & $0.24 \pm 0.003$ & $0.13 \pm 0.014$ & $0.37 \pm 0.002$ \\

    \texttt{RF} & $0.39 \pm 0.005$ & $0.83 \pm 0.004$ & $0.27 \pm 0.008$ & $0.07 \pm 0.014$ & $0.42 \pm 0.006$\\

    \texttt{kNN} & $0.31 \pm 0.011$ & $0.56 \pm 0.004$ & $0.22 \pm 0.09$ & $0.20 \pm 0.003$ & $0.33 \pm 0.009$ \\

    \hline
  \end{tabular}
  \caption{Performance statistics sequence-to-sequence features on a multiclass classification task with k-fold validation whilst training on $10\%$ of the unmonitored pages.}
  \label{table:mult-seq2seq-test-error}
\end{table}

\begin{figure}[ht]
  \centering
  \includegraphics[width=\textwidth]{mult-unmon-performance}
  \caption{Varying the amount of unmonitored pages trained on for different features.}
  \label{fig:mult-unmon-performance}
\end{figure}

But the behavior of the classifiers gets even more interesting when training on a larger amount of unmonitored pages.
Figure \ref{fig:mult-unmon-performance} shows that the stacked autoencoder has a very similar performance as on the binary classification task.
Whilst the sequence-to-sequence model performs almost as well as the hand-picked features.
In fact, the sequence-to-sequence features even perform better on the \texttt{svc}.

\subsubsection{Different Circumstances}

Beside analysing how the fingerprint extraction models perform on data within the same dataset, it would be interesting to examine how it performs on data recorded under different circumstances.
It has already been shown that the performance of the classifiers is greatly impacted by the network, time and the TBB version.
But that doesn't necessarily mean that our fingerprint extraction model is impacted similarly.

If the deep learning models are not impacted by these flaws, an adversary would only need to train the fingerprint extraction model once and then it could continue to use it and only retrain the classifiers, like some sort of \textit{transfer learning}

To test this premise, we use the models that we previously trained on the same $120,000$ web pages within the \texttt{GRESCHBACH} dataset.
More specifically, a LSTM bidirectional encoder with $80$ hidden states and a stacked autoencoder with sizes of the hidden layers being $3000$, $1600$ and $200$.
Next, we extract the fingerprints from the Tor cells within the \texttt{WANG14} dataset using this model, train a set of classifiers on these fingerprints using k-fold validation and note down their performance.

For the following experiments, we train the classifiers using $k = 3$ and for both datasets, we  pick a total $100$ monitored web pages with $70$ instances each and $5000$ unmonitored web pages.

\begin{figure}[ht]
  \centering
  \includegraphics[width=\textwidth]{different-circumstances}
  \caption{Classifier performance on the WANG14 dataset with automatically extracted features.}
  \label{fig:different-circumstances}
\end{figure}

Figure \ref{fig:different-circumstances} shows us that the performance drops slightly when extracting features on the \texttt{WANG14} dataset.
Especially with the stacked autoencoder.
However, the sequence-to-sequence model seems to achieve similar F1 scores on both the \texttt{GRESCHBACH} and \texttt{WANG14} dataset.

\section{Unit Tests}

On top of evaluating the results, we also needed to ensure that the code behaves as we expect it to.
For this we use unit tests.
Some bits of the code, such as the Tensorflow models are difficult to test but we can still test all of the preprocessing to see the correct values are produced.
For this we use Python's standard \texttt{unittest} module \cite{python_unittest_documentation}.
The reason for this choice is that it is flexible and the standard Python unit testing framework, which means it is commonly used.

On top of unit tests, \textit{Travis} was also used \cite{travis}.
Travis is a popular tool, that has an easy integration with Github, for continuous integration.
Therefore, every time a commit is pushed to the remote repository, Travis runs all of the tests automatically.
If one of the tests fails, Travis then automatically notifies all the contributors.

Finally, to check if our tests cover our entire codebase, \textit{codecov} is used \cite{codecov}.
This tool automatically checks how much of the codebase all of the unit tests cover.
At the time of writing, the coverage is $93\%$.
The bits that aren't covered by unit tests, such as the Tensorflow implementation of the deep learning models, have been carefully examined to see if they behave as expected by using the Tensorflow debugger \cite{tensorflow}.
